{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse jupyter notebook serve para transformar os dados brutos zipados em arquivos agregados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bolsa Família"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de 2020-01 até 2021-10\n",
    "\n",
    "file_paths_bf = [    \n",
    "    'raw_data/bolsa_familia/201908',\n",
    "    'raw_data/bolsa_familia/201909',\n",
    "    'raw_data/bolsa_familia/201910',\n",
    "    'raw_data/bolsa_familia/201911',\n",
    "    'raw_data/bolsa_familia/201912',\n",
    "    'raw_data/bolsa_familia/202001',\n",
    "    'raw_data/bolsa_familia/202002',\n",
    "    'raw_data/bolsa_familia/202003',\n",
    "    'raw_data/bolsa_familia/202004',\n",
    "    'raw_data/bolsa_familia/202005',\n",
    "    'raw_data/bolsa_familia/202006',\n",
    "    'raw_data/bolsa_familia/202007',\n",
    "    'raw_data/bolsa_familia/202008',\n",
    "    'raw_data/bolsa_familia/202009',\n",
    "    'raw_data/bolsa_familia/202010',\n",
    "    'raw_data/bolsa_familia/202011',\n",
    "    'raw_data/bolsa_familia/202012',\n",
    "    'raw_data/bolsa_familia/202101',\n",
    "    'raw_data/bolsa_familia/202102',\n",
    "    'raw_data/bolsa_familia/202103',\n",
    "    'raw_data/bolsa_familia/202104',\n",
    "    'raw_data/bolsa_familia/202105',\n",
    "    'raw_data/bolsa_familia/202106',\n",
    "    'raw_data/bolsa_familia/202107',\n",
    "    'raw_data/bolsa_familia/202108',\n",
    "    'raw_data/bolsa_familia/202109',\n",
    "    'raw_data/bolsa_familia/202110',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip_file_bf(path):\n",
    "    if os.path.exists(path):\n",
    "        with zipfile.ZipFile(path, 'r') as zip_ref:\n",
    "            zip_ref.extractall('raw_data/bolsa_familia/')\n",
    "    else:\n",
    "        print(f\"File {path} doesn't exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agg_chunks_bf(path_mes):\n",
    "    tamanho_total = 0\n",
    "    with pd.read_csv('raw_data/bolsa_familia/' + path_mes + '_BolsaFamilia_Saques.csv', chunksize=500_000, sep=';', encoding='latin-1') as reader:\n",
    "            clean_chunks = []\n",
    "            for df in reader:\n",
    "                tamanho_total += len(df)\n",
    "                df['VALOR PARCELA'] = df['VALOR PARCELA'].str.replace(',','.')\n",
    "                df['VALOR PARCELA'] = df['VALOR PARCELA'].astype(float)\n",
    "\n",
    "                colunas = ['CÓDIGO MUNICÍPIO SIAFI', 'VALOR PARCELA']\n",
    "\n",
    "                valores_soma = df[colunas].groupby(['CÓDIGO MUNICÍPIO SIAFI'],dropna=False).sum().reset_index()\n",
    "                valores_count = df[colunas].groupby(['CÓDIGO MUNICÍPIO SIAFI'],dropna=False).size().reset_index()\n",
    "\n",
    "                df_merge = valores_soma.merge(valores_count, on='CÓDIGO MUNICÍPIO SIAFI', how='outer')\n",
    "                df_merge.columns = ['municipio_siafi', 'soma', 'contagem']\n",
    "                clean_chunks.append(df_merge)\n",
    "    return clean_chunks, tamanho_total\n",
    "\n",
    "def merge_chunks_bf(clean_chunks):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for chunk in clean_chunks:\n",
    "        if df.empty:\n",
    "            df = chunk\n",
    "        else:\n",
    "            df_merge = df.merge(chunk, on='municipio_siafi', how='outer')\n",
    "            assert len(df_merge) <= len(df) + len(chunk)\n",
    "            df_merge = df_merge.fillna(0)\n",
    "            df_merge['contagem'] = df_merge['contagem_x'] + df_merge['contagem_y']\n",
    "            df_merge['soma'] = df_merge['soma_x'] + df_merge['soma_y']\n",
    "            df_merge = df_merge.drop(['soma_x', 'soma_y', 'contagem_x', 'contagem_y'], axis=1)\n",
    "            df = df_merge\n",
    "    return df\n",
    "\n",
    "def transform_file_bf(path):\n",
    "    path_mes = path.split('/')[-1]\n",
    "\n",
    "    # Check if transformed file already exists\n",
    "    file_destination_name = 'parsed_data/bolsa_familia/' + path_mes + '.csv'\n",
    "\n",
    "    if os.path.exists(file_destination_name):\n",
    "        print(f'Arquivo {file_destination_name} já existente, pulando transformação.')\n",
    "        return\n",
    "    \n",
    "    # Check if extracted file already exists\n",
    "    extracted_file = 'raw_data/bolsa_familia/' + path_mes + '_BolsaFamilia_Saques.csv'\n",
    "    if not os.path.exists(extracted_file):\n",
    "        print(f'Extraindo {path}.')\n",
    "        extract_zip_file_bf(path)\n",
    "        print(f'Concluído.')\n",
    "    else:  \n",
    "        print(f'Dados já extraídos, reaproveitando {extracted_file}.')\n",
    "    \n",
    "    print(f'Agregando dados.')\n",
    "\n",
    "    clean_chunks, tamanho_total = create_agg_chunks_bf(path_mes)\n",
    "\n",
    "    print(f'tamanho total: {tamanho_total}')\n",
    "\n",
    "    df_final = merge_chunks_bf(clean_chunks)\n",
    "\n",
    "    assert df_final['contagem'].sum() == tamanho_total\n",
    "\n",
    "    print(f'Concluído.')\n",
    "    df_final.to_csv(file_destination_name, index=False)\n",
    "    print(f'Arquivo salvo em {file_destination_name}.')\n",
    "\n",
    "    # Clean extracted file in the end\n",
    "    print(f'Apagando csv extraído.')\n",
    "    os.remove(extracted_file)\n",
    "    print(f'Concluído.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo parsed_data/bolsa_familia/201908.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/201909.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/201910.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/201911.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/201912.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202001.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202002.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202003.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202004.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202005.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202006.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202007.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202008.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202009.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202010.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202011.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202012.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202101.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202102.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202103.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202104.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202105.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202106.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202107.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202108.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202109.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202110.csv já existente, pulando transformação.\n"
     ]
    }
   ],
   "source": [
    "for file in file_paths_bf:\n",
    "    transform_file_bf(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxílio Emergencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de 2020-01 até 2021-10\n",
    "\n",
    "file_paths_ae = [    \n",
    "    'raw_data/auxilio_emergencial/202206',\n",
    "    'raw_data/auxilio_emergencial/202205',\n",
    "    'raw_data/auxilio_emergencial/202204',\n",
    "    'raw_data/auxilio_emergencial/202203',\n",
    "    'raw_data/auxilio_emergencial/202202',\n",
    "    'raw_data/auxilio_emergencial/202201',\n",
    "    'raw_data/auxilio_emergencial/202112',\n",
    "    'raw_data/auxilio_emergencial/202111',\n",
    "    'raw_data/auxilio_emergencial/202110',\n",
    "    'raw_data/auxilio_emergencial/202109',\n",
    "    'raw_data/auxilio_emergencial/202108',\n",
    "    'raw_data/auxilio_emergencial/202107',\n",
    "    'raw_data/auxilio_emergencial/202106',\n",
    "    'raw_data/auxilio_emergencial/202105',\n",
    "    'raw_data/auxilio_emergencial/202104',\n",
    "    'raw_data/auxilio_emergencial/202103',\n",
    "    'raw_data/auxilio_emergencial/202102',\n",
    "    'raw_data/auxilio_emergencial/202101',\n",
    "    'raw_data/auxilio_emergencial/202012',\n",
    "    'raw_data/auxilio_emergencial/202011',\n",
    "    'raw_data/auxilio_emergencial/202010',\n",
    "    'raw_data/auxilio_emergencial/202009',\n",
    "    'raw_data/auxilio_emergencial/202008',\n",
    "    'raw_data/auxilio_emergencial/202007',\n",
    "    'raw_data/auxilio_emergencial/202006',\n",
    "    'raw_data/auxilio_emergencial/202005',\n",
    "    'raw_data/auxilio_emergencial/202004',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip_file_ae(path):\n",
    "    if os.path.exists(path):\n",
    "        with zipfile.ZipFile(path, 'r') as zip_ref:\n",
    "            zip_ref.extractall('raw_data/auxilio_emergencial/')\n",
    "    else:\n",
    "        print(f\"File {path} doesn't exist.\")\n",
    "\n",
    "def create_agg_chunks_ae(path_mes):\n",
    "    tamanho_total = 0\n",
    "    with pd.read_csv('raw_data/auxilio_emergencial/' + path_mes + '_AuxilioEmergencial.csv', chunksize=500_000, sep=';', encoding='latin-1') as reader:\n",
    "            clean_chunks = []\n",
    "            for df in reader:\n",
    "                df['VALOR BENEFÍCIO'] = df['VALOR BENEFÍCIO'].str.replace(',','.')\n",
    "                df['VALOR BENEFÍCIO'] = df['VALOR BENEFÍCIO'].astype(float)\n",
    "\n",
    "                obs_filter = (df['OBSERVAÇÃO'] == 'Não há') | (df['OBSERVAÇÃO'].isna())\n",
    "\n",
    "                df = df[obs_filter]\n",
    "                \n",
    "                tamanho_total += len(df)\n",
    "\n",
    "                colunas = ['CÓDIGO MUNICÍPIO IBGE', 'VALOR BENEFÍCIO', 'NOME BENEFICIÁRIO']\n",
    "\n",
    "                valores_soma = df[colunas].groupby(['CÓDIGO MUNICÍPIO IBGE'],dropna=False).sum().reset_index()\n",
    "                valores_count = df[colunas].groupby(['CÓDIGO MUNICÍPIO IBGE', 'NOME BENEFICIÁRIO'], dropna=False).size().groupby(['CÓDIGO MUNICÍPIO IBGE']).size().reset_index()\n",
    "\n",
    "                df_merge = valores_soma.merge(valores_count, on='CÓDIGO MUNICÍPIO IBGE', how='outer')\n",
    "                df_merge.columns = ['municipio_ibge', 'soma', 'contagem']\n",
    "                clean_chunks.append(df_merge)\n",
    "    return clean_chunks, tamanho_total\n",
    "\n",
    "def merge_chunks_ae(clean_chunks):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for chunk in clean_chunks:\n",
    "        if df.empty:\n",
    "            df = chunk\n",
    "        else:\n",
    "            df_merge = df.merge(chunk, on='municipio_ibge', how='outer')\n",
    "            assert len(df_merge) <= len(df) + len(chunk)\n",
    "            df_merge = df_merge.fillna(0)\n",
    "            df_merge['contagem'] = df_merge['contagem_x'] + df_merge['contagem_y']\n",
    "            df_merge['soma'] = df_merge['soma_x'] + df_merge['soma_y']\n",
    "            df_merge = df_merge.drop(['soma_x', 'soma_y', 'contagem_x', 'contagem_y'], axis=1)\n",
    "            df = df_merge\n",
    "    return df\n",
    "\n",
    "def transform_file_ae(path):\n",
    "    path_mes = path.split('/')[-1]\n",
    "\n",
    "    # Check if transformed file already exists\n",
    "    file_destination_name = 'parsed_data/auxilio_emergencial/' + path_mes + '.csv'\n",
    "\n",
    "    if os.path.exists(file_destination_name):\n",
    "        print(f'Arquivo {file_destination_name} já existente, pulando.. transformação.')\n",
    "        return\n",
    "    \n",
    "    # Check if extracted file already exists\n",
    "    extracted_file = 'raw_data/auxilio_emergencial/' + path_mes + '_AuxilioEmergencial.csv'\n",
    "    if not os.path.exists(extracted_file):\n",
    "        print(f'Extraindo {path}.')\n",
    "        extract_zip_file_ae(path)\n",
    "        print(f'Concluído.')\n",
    "    else:  \n",
    "        print(f'Dados já extraídos, reaproveitando {extracted_file}.')\n",
    "    \n",
    "    print(f'Agregando dados.')\n",
    "\n",
    "    clean_chunks, tamanho_total = create_agg_chunks_ae(path_mes)\n",
    "\n",
    "    print(f'tamanho total: {tamanho_total}')\n",
    "\n",
    "    df_final = merge_chunks_ae(clean_chunks)\n",
    "\n",
    "    print(f'Concluído.')\n",
    "    df_final.to_csv(file_destination_name, index=False)\n",
    "    print(f'Arquivo salvo em {file_destination_name}.')\n",
    "\n",
    "    # Clean extracted file in the end\n",
    "    print(f'Apagando csv extraído.')\n",
    "    os.remove(extracted_file)\n",
    "    print(f'Concluído.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo parsed_data/auxilio_emergencial/202206.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202205.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202204.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202203.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202202.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202201.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202112.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202111.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202110.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202109.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202108.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202107.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202106.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202105.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202104.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202103.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202102.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202101.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202012.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202011.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202010.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202009.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202008.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202007.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202006.csv já existente, pulando.. transformação.\n",
      "Dados já extraídos, reaproveitando raw_data/auxilio_emergencial/202005_AuxilioEmergencial.csv.\n",
      "Agregando dados.\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: EOF inside string starting at row 8463960",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2434386/2970323296.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_paths_ae\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtransform_file_ae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2434386/270003330.py\u001b[0m in \u001b[0;36mtransform_file_ae\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Agregando dados.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mclean_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtamanho_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_agg_chunks_ae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_mes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'tamanho total: {tamanho_total}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2434386/270003330.py\u001b[0m in \u001b[0;36mcreate_agg_chunks_ae\u001b[0;34m(path_mes)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'raw_data/auxilio_emergencial/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpath_mes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_AuxilioEmergencial.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500_000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mclean_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'VALOR BENEFÍCIO'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'VALOR BENEFÍCIO'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'VALOR BENEFÍCIO'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'VALOR BENEFÍCIO'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mget_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1072\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_currow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 8463960"
     ]
    }
   ],
   "source": [
    "for file in file_paths_ae:\n",
    "    transform_file_ae(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
