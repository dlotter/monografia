{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse jupyter notebook serve para transformar os dados brutos zipados em arquivos agregados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bolsa Família"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de 2020-01 até 2021-10\n",
    "\n",
    "file_paths_bf = [    \n",
    "    'raw_data/files/bolsa_familia/201908',\n",
    "    'raw_data/files/bolsa_familia/201909',\n",
    "    'raw_data/files/bolsa_familia/201910',\n",
    "    'raw_data/files/bolsa_familia/201911',\n",
    "    'raw_data/files/bolsa_familia/201912',\n",
    "    'raw_data/files/bolsa_familia/202001',\n",
    "    'raw_data/files/bolsa_familia/202002',\n",
    "    'raw_data/files/bolsa_familia/202003',\n",
    "    'raw_data/files/bolsa_familia/202004',\n",
    "    'raw_data/files/bolsa_familia/202005',\n",
    "    'raw_data/files/bolsa_familia/202006',\n",
    "    'raw_data/files/bolsa_familia/202007',\n",
    "    'raw_data/files/bolsa_familia/202008',\n",
    "    'raw_data/files/bolsa_familia/202009',\n",
    "    'raw_data/files/bolsa_familia/202010',\n",
    "    'raw_data/files/bolsa_familia/202011',\n",
    "    'raw_data/files/bolsa_familia/202012',\n",
    "    'raw_data/files/bolsa_familia/202101',\n",
    "    'raw_data/files/bolsa_familia/202102',\n",
    "    'raw_data/files/bolsa_familia/202103',\n",
    "    'raw_data/files/bolsa_familia/202104',\n",
    "    'raw_data/files/bolsa_familia/202105',\n",
    "    'raw_data/files/bolsa_familia/202106',\n",
    "    'raw_data/files/bolsa_familia/202107',\n",
    "    'raw_data/files/bolsa_familia/202108',\n",
    "    'raw_data/files/bolsa_familia/202109',\n",
    "    'raw_data/files/bolsa_familia/202110',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip_file_bf(path):\n",
    "    if os.path.exists(path):\n",
    "        with zipfile.ZipFile(path, 'r') as zip_ref:\n",
    "            zip_ref.extractall('raw_data/bolsa_familia/')\n",
    "    else:\n",
    "        print(f\"File {path} doesn't exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agg_chunks_bf(path_mes):\n",
    "    tamanho_total = 0\n",
    "    with pd.read_csv('raw_data/bolsa_familia/' + path_mes + '_BolsaFamilia_Pagamentos.csv', chunksize=500_000, sep=';', encoding='latin-1') as reader:\n",
    "            clean_chunks = []\n",
    "            for df in reader:\n",
    "                tamanho_total += len(df)\n",
    "                df['VALOR PARCELA'] = df['VALOR PARCELA'].str.replace(',','.')\n",
    "                df['VALOR PARCELA'] = df['VALOR PARCELA'].astype(float)\n",
    "\n",
    "                colunas = ['CÓDIGO MUNICÍPIO SIAFI', 'VALOR PARCELA', 'NOME FAVORECIDO']\n",
    "\n",
    "                valores_soma = df[colunas].groupby(['CÓDIGO MUNICÍPIO SIAFI'],dropna=False).sum(numeric_only=True).reset_index()\n",
    "                valores_count = df[colunas].groupby(['CÓDIGO MUNICÍPIO SIAFI', \"NOME FAVORECIDO\"],dropna=False).size().groupby(['CÓDIGO MUNICÍPIO SIAFI']).size().reset_index()\n",
    "\n",
    "                df_merge = valores_soma.merge(valores_count, on='CÓDIGO MUNICÍPIO SIAFI', how='outer')\n",
    "                df_merge.columns = ['municipio_siafi', 'soma', 'contagem']\n",
    "                clean_chunks.append(df_merge)\n",
    "    return clean_chunks, tamanho_total\n",
    "\n",
    "def merge_chunks_bf(clean_chunks):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for chunk in clean_chunks:\n",
    "        if df.empty:\n",
    "            df = chunk\n",
    "        else:\n",
    "            df_merge = df.merge(chunk, on='municipio_siafi', how='outer')\n",
    "            assert len(df_merge) <= len(df) + len(chunk)\n",
    "            df_merge = df_merge.fillna(0)\n",
    "            df_merge['contagem'] = df_merge['contagem_x'] + df_merge['contagem_y']\n",
    "            df_merge['soma'] = df_merge['soma_x'] + df_merge['soma_y']\n",
    "            df_merge = df_merge.drop(['soma_x', 'soma_y', 'contagem_x', 'contagem_y'], axis=1)\n",
    "            df = df_merge\n",
    "    return df\n",
    "\n",
    "def transform_file_bf(path):\n",
    "    path_mes = path.split('/')[-1]\n",
    "\n",
    "    # Check if transformed file already exists\n",
    "    file_destination_name = 'parsed_data/bolsa_familia/' + path_mes + '.csv'\n",
    "\n",
    "    if os.path.exists(file_destination_name):\n",
    "        print(f'Arquivo {file_destination_name} já existente, pulando transformação.')\n",
    "        return\n",
    "    \n",
    "    # Check if extracted file already exists\n",
    "    extracted_file = 'raw_data/bolsa_familia/' + path_mes + '_BolsaFamilia_Pagamentos.csv'\n",
    "    if not os.path.exists(extracted_file):\n",
    "        print(f'Extraindo {path}.')\n",
    "        extract_zip_file_bf(path)\n",
    "        print(f'Concluído.')\n",
    "    else:  \n",
    "        print(f'Dados já extraídos, reaproveitando {extracted_file}.')\n",
    "    \n",
    "    print(f'Agregando dados.')\n",
    "\n",
    "    clean_chunks, tamanho_total = create_agg_chunks_bf(path_mes)\n",
    "\n",
    "    print(f'tamanho total: {tamanho_total}')\n",
    "\n",
    "    df_final = merge_chunks_bf(clean_chunks)\n",
    "\n",
    "    print(f'Concluído.')\n",
    "    df_final.to_csv(file_destination_name, index=False)\n",
    "    print(f'Arquivo salvo em {file_destination_name}.')\n",
    "\n",
    "    # Clean extracted file in the end\n",
    "    print(f'Apagando csv extraído.')\n",
    "    os.remove(extracted_file)\n",
    "    print(f'Concluído.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo parsed_data/bolsa_familia/201908.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/201909.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/201910.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/201911.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/201912.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202001.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202002.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202003.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202004.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202005.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202006.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202007.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202008.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202009.csv já existente, pulando transformação.\n",
      "Arquivo parsed_data/bolsa_familia/202010.csv já existente, pulando transformação.\n",
      "Extraindo raw_data/bolsa_familia/202011.\n",
      "Concluído.\n",
      "Agregando dados.\n",
      "tamanho total: 14273799\n",
      "Concluído.\n",
      "Arquivo salvo em parsed_data/bolsa_familia/202011.csv.\n",
      "Apagando csv extraído.\n",
      "Concluído.\n",
      "Extraindo raw_data/bolsa_familia/202012.\n",
      "Concluído.\n",
      "Agregando dados.\n",
      "tamanho total: 14274019\n",
      "Concluído.\n",
      "Arquivo salvo em parsed_data/bolsa_familia/202012.csv.\n",
      "Apagando csv extraído.\n",
      "Concluído.\n",
      "Extraindo raw_data/bolsa_familia/202101.\n",
      "Concluído.\n",
      "Agregando dados.\n",
      "tamanho total: 14233117\n",
      "Concluído.\n",
      "Arquivo salvo em parsed_data/bolsa_familia/202101.csv.\n",
      "Apagando csv extraído.\n",
      "Concluído.\n",
      "Extraindo raw_data/bolsa_familia/202102.\n",
      "Concluído.\n",
      "Agregando dados.\n",
      "tamanho total: 14266042\n",
      "Concluído.\n",
      "Arquivo salvo em parsed_data/bolsa_familia/202102.csv.\n",
      "Apagando csv extraído.\n",
      "Concluído.\n",
      "Extraindo raw_data/bolsa_familia/202103.\n",
      "Concluído.\n",
      "Agregando dados.\n",
      "tamanho total: 14524589\n",
      "Concluído.\n",
      "Arquivo salvo em parsed_data/bolsa_familia/202103.csv.\n",
      "Apagando csv extraído.\n",
      "Concluído.\n",
      "Extraindo raw_data/bolsa_familia/202104.\n",
      "Concluído.\n",
      "Agregando dados.\n",
      "tamanho total: 14613280\n",
      "Concluído.\n",
      "Arquivo salvo em parsed_data/bolsa_familia/202104.csv.\n",
      "Apagando csv extraído.\n",
      "Concluído.\n",
      "Extraindo raw_data/bolsa_familia/202105.\n",
      "Concluído.\n",
      "Agregando dados.\n",
      "tamanho total: 14695132\n",
      "Concluído.\n",
      "Arquivo salvo em parsed_data/bolsa_familia/202105.csv.\n",
      "Apagando csv extraído.\n",
      "Concluído.\n",
      "Extraindo raw_data/bolsa_familia/202106.\n",
      "Concluído.\n",
      "Agregando dados.\n",
      "tamanho total: 14694799\n",
      "Concluído.\n",
      "Arquivo salvo em parsed_data/bolsa_familia/202106.csv.\n",
      "Apagando csv extraído.\n",
      "Concluído.\n",
      "Extraindo raw_data/bolsa_familia/202107.\n",
      "Concluído.\n",
      "Agregando dados.\n",
      "tamanho total: 14694737\n",
      "Concluído.\n",
      "Arquivo salvo em parsed_data/bolsa_familia/202107.csv.\n",
      "Apagando csv extraído.\n",
      "Concluído.\n",
      "Extraindo raw_data/bolsa_familia/202108.\n",
      "Concluído.\n",
      "Agregando dados.\n",
      "tamanho total: 14655448\n",
      "Concluído.\n",
      "Arquivo salvo em parsed_data/bolsa_familia/202108.csv.\n",
      "Apagando csv extraído.\n",
      "Concluído.\n",
      "Extraindo raw_data/bolsa_familia/202109.\n",
      "Concluído.\n",
      "Agregando dados.\n",
      "tamanho total: 14655291\n",
      "Concluído.\n",
      "Arquivo salvo em parsed_data/bolsa_familia/202109.csv.\n",
      "Apagando csv extraído.\n",
      "Concluído.\n",
      "Extraindo raw_data/bolsa_familia/202110.\n",
      "Concluído.\n",
      "Agregando dados.\n",
      "tamanho total: 14654789\n",
      "Concluído.\n",
      "Arquivo salvo em parsed_data/bolsa_familia/202110.csv.\n",
      "Apagando csv extraído.\n",
      "Concluído.\n"
     ]
    }
   ],
   "source": [
    "for file in file_paths_bf:\n",
    "    transform_file_bf(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxílio Emergencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de 2020-01 até 2021-10\n",
    "\n",
    "file_paths_ae = [    \n",
    "    'raw_data/files/auxilio_emergencial/202206',\n",
    "    'raw_data/files/auxilio_emergencial/202205',\n",
    "    'raw_data/files/auxilio_emergencial/202204',\n",
    "    'raw_data/files/auxilio_emergencial/202203',\n",
    "    'raw_data/files/auxilio_emergencial/202202',\n",
    "    'raw_data/files/auxilio_emergencial/202201',\n",
    "    'raw_data/files/auxilio_emergencial/202112',\n",
    "    'raw_data/files/auxilio_emergencial/202111',\n",
    "    'raw_data/files/auxilio_emergencial/202110',\n",
    "    'raw_data/files/auxilio_emergencial/202109',\n",
    "    'raw_data/files/auxilio_emergencial/202108',\n",
    "    'raw_data/files/auxilio_emergencial/202107',\n",
    "    'raw_data/files/auxilio_emergencial/202106',\n",
    "    'raw_data/files/auxilio_emergencial/202105',\n",
    "    'raw_data/files/auxilio_emergencial/202104',\n",
    "    'raw_data/files/auxilio_emergencial/202103',\n",
    "    'raw_data/files/auxilio_emergencial/202102',\n",
    "    'raw_data/files/auxilio_emergencial/202101',\n",
    "    'raw_data/files/auxilio_emergencial/202012',\n",
    "    'raw_data/files/auxilio_emergencial/202011',\n",
    "    'raw_data/files/auxilio_emergencial/202010',\n",
    "    'raw_data/files/auxilio_emergencial/202009',\n",
    "    'raw_data/files/auxilio_emergencial/202008',\n",
    "    'raw_data/files/auxilio_emergencial/202007',\n",
    "    'raw_data/files/auxilio_emergencial/202006',\n",
    "    'raw_data/files/auxilio_emergencial/202005',\n",
    "    'raw_data/files/auxilio_emergencial/202004',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip_file_ae(path):\n",
    "    if os.path.exists(path):\n",
    "        with zipfile.ZipFile(path, 'r') as zip_ref:\n",
    "            zip_ref.extractall('raw_data/auxilio_emergencial/')\n",
    "    else:\n",
    "        print(f\"File {path} doesn't exist.\")\n",
    "\n",
    "def create_agg_chunks_ae(path_mes):\n",
    "    tamanho_total = 0\n",
    "    with pd.read_csv('raw_data/auxilio_emergencial/' + path_mes + '_AuxilioEmergencial.csv', chunksize=500_000, sep=';', encoding='latin-1') as reader:\n",
    "            clean_chunks = []\n",
    "            for df in reader:\n",
    "                df['VALOR BENEFÍCIO'] = df['VALOR BENEFÍCIO'].str.replace(',','.')\n",
    "                df['VALOR BENEFÍCIO'] = df['VALOR BENEFÍCIO'].astype(float)\n",
    "\n",
    "                obs_filter = (df['OBSERVAÇÃO'] == 'Não há') | (df['OBSERVAÇÃO'].isna())\n",
    "\n",
    "                df = df[obs_filter]\n",
    "                \n",
    "                tamanho_total += len(df)\n",
    "\n",
    "                colunas = ['CÓDIGO MUNICÍPIO IBGE', 'VALOR BENEFÍCIO', 'NOME BENEFICIÁRIO']\n",
    "\n",
    "                valores_soma = df[colunas].groupby(['CÓDIGO MUNICÍPIO IBGE'],dropna=False).sum().reset_index()\n",
    "                valores_count = df[colunas].groupby(['CÓDIGO MUNICÍPIO IBGE', 'NOME BENEFICIÁRIO'], dropna=False).size().groupby(['CÓDIGO MUNICÍPIO IBGE']).size().reset_index()\n",
    "\n",
    "                df_merge = valores_soma.merge(valores_count, on='CÓDIGO MUNICÍPIO IBGE', how='outer')\n",
    "                df_merge.columns = ['municipio_ibge', 'soma', 'contagem']\n",
    "                clean_chunks.append(df_merge)\n",
    "    return clean_chunks, tamanho_total\n",
    "\n",
    "def merge_chunks_ae(clean_chunks):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for chunk in clean_chunks:\n",
    "        if df.empty:\n",
    "            df = chunk\n",
    "        else:\n",
    "            df_merge = df.merge(chunk, on='municipio_ibge', how='outer')\n",
    "            assert len(df_merge) <= len(df) + len(chunk)\n",
    "            df_merge = df_merge.fillna(0)\n",
    "            df_merge['contagem'] = df_merge['contagem_x'] + df_merge['contagem_y']\n",
    "            df_merge['soma'] = df_merge['soma_x'] + df_merge['soma_y']\n",
    "            df_merge = df_merge.drop(['soma_x', 'soma_y', 'contagem_x', 'contagem_y'], axis=1)\n",
    "            df = df_merge\n",
    "    return df\n",
    "\n",
    "def transform_file_ae(path):\n",
    "    path_mes = path.split('/')[-1]\n",
    "\n",
    "    # Check if transformed file already exists\n",
    "    file_destination_name = 'parsed_data/auxilio_emergencial/' + path_mes + '.csv'\n",
    "\n",
    "    if os.path.exists(file_destination_name):\n",
    "        print(f'Arquivo {file_destination_name} já existente, pulando.. transformação.')\n",
    "        return\n",
    "    \n",
    "    # Check if extracted file already exists\n",
    "    extracted_file = 'raw_data/auxilio_emergencial/' + path_mes + '_AuxilioEmergencial.csv'\n",
    "    if not os.path.exists(extracted_file):\n",
    "        print(f'Extraindo {path}.')\n",
    "        extract_zip_file_ae(path)\n",
    "        print(f'Concluído.')\n",
    "    else:  \n",
    "        print(f'Dados já extraídos, reaproveitando {extracted_file}.')\n",
    "    \n",
    "    print(f'Agregando dados.')\n",
    "\n",
    "    clean_chunks, tamanho_total = create_agg_chunks_ae(path_mes)\n",
    "\n",
    "    print(f'tamanho total: {tamanho_total}')\n",
    "\n",
    "    df_final = merge_chunks_ae(clean_chunks)\n",
    "\n",
    "    print(f'Concluído.')\n",
    "    df_final.to_csv(file_destination_name, index=False)\n",
    "    print(f'Arquivo salvo em {file_destination_name}.')\n",
    "\n",
    "    # Clean extracted file in the end\n",
    "    print(f'Apagando csv extraído.')\n",
    "    os.remove(extracted_file)\n",
    "    print(f'Concluído.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo parsed_data/auxilio_emergencial/202206.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202205.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202204.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202203.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202202.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202201.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202112.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202111.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202110.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202109.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202108.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202107.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202106.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202105.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202104.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202103.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202102.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202101.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202012.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202011.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202010.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202009.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202008.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202007.csv já existente, pulando.. transformação.\n",
      "Arquivo parsed_data/auxilio_emergencial/202006.csv já existente, pulando.. transformação.\n",
      "Extraindo raw_data/auxilio_emergencial/202005.\n",
      "Concluído.\n",
      "Agregando dados.\n",
      "tamanho total: 55525879\n",
      "Concluído.\n",
      "Arquivo salvo em parsed_data/auxilio_emergencial/202005.csv.\n",
      "Apagando csv extraído.\n",
      "Concluído.\n",
      "Extraindo raw_data/auxilio_emergencial/202004.\n",
      "Concluído.\n",
      "Agregando dados.\n",
      "tamanho total: 45473004\n",
      "Concluído.\n",
      "Arquivo salvo em parsed_data/auxilio_emergencial/202004.csv.\n",
      "Apagando csv extraído.\n",
      "Concluído.\n"
     ]
    }
   ],
   "source": [
    "for file in file_paths_ae:\n",
    "    transform_file_ae(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74ecc8fd3aec51f280785299e87690e61bb7821f3069ae3f92509e7e4de76ad8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
